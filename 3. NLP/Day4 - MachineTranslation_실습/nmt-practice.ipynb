{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Example\n",
    "from mosestokenizer import *\n",
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Author: WonKee Lee (POSTECH)\n",
    "# \"Neural Machine Translation by Jointly Learning to Align and Translate\" 논문의 model 재현 (Toy code)\n",
    "#  (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html 를 참고하여 수정함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torchtext #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'    # Start symbol\n",
    "EOS = '</s>'   # End symbol\n",
    "PAD = '<pad>'  # padding symbol\n",
    "\n",
    "# ex) 'I am a boy.' -> ['I', 'am', 'a', 'boy']\n",
    "tok_en = MosesTokenizer('en')\n",
    "tok_fr = MosesTokenizer('fr')\n",
    "\n",
    "# Field: Tensor로 표현할 데이터의 타입, 처리 프로세스 등을 정의하는 객체\n",
    "src = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_en,\n",
    "            lower=True,\n",
    "            batch_first=True) # if=True shape:[Batch, length] else shape=[length, Batch]\n",
    "\n",
    "tgt = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tok_fr,\n",
    "            lower=True,\n",
    "            init_token=BOS,\n",
    "            eos_token=EOS,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_f = 'data/data'\n",
    "\n",
    "# parallel data 각각 (en, de) 을 src Field 와 tgt Field에 정의된 형태로 처리.\n",
    "parallel_dataset = TranslationDataset(path=prefix_f, exts=('.en', '.fr'), \n",
    "                                      fields=[('src', src), ('tgt', tgt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.translation.TranslationDataset object at 0x7f09a96e4160>\n",
      "dict_items([('src', ['you', 'were', 'in', 'a', 'coma', '.']), ('tgt', ['tu', 'étais', 'dans', 'le', 'coma', '.'])])\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset) \n",
    "\n",
    "print(parallel_dataset.examples[22222].__dict__.items()) # src 및 tgt 에 대한 samples 를 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'were', 'in', 'a', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].src) # src 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tu', 'étais', 'dans', 'le', 'coma', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[22222].tgt) # tgt 출력 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 사전 구축 ########\n",
    "# src, tgt 필드에 사전 구축\n",
    "src.build_vocab(parallel_dataset, max_size=15000)\n",
    "tgt.build_vocab(parallel_dataset, max_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "\n",
      "     <unk> |   0\n",
      "     <pad> |   1\n",
      "         . |   2\n",
      "         i |   3\n",
      "       you |   4\n",
      "        to |   5\n",
      "       the |   6\n",
      "         ? |   7\n",
      "         a |   8\n",
      "   &apos;t |   9\n",
      "        is |  10\n",
      "        it |  11\n",
      "        he |  12\n",
      "      that |  13\n",
      "   &apos;s |  14\n",
      "        of |  15\n"
     ]
    }
   ],
   "source": [
    "# 사전 내용 \n",
    "print(src.vocab.__dict__.keys())\n",
    "print('')\n",
    "# stoi : string to index 의 약자\n",
    "for i, (k, v) in enumerate(src.vocab.stoi.items()):\n",
    "    print ('{:>10s} | {:>3d}'.format(k, v))\n",
    "    if i == 15 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = parallel_dataset.split(split_ratio=0.95) # 0.95 = train / 0.05 = valid 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iterator 생성.\n",
    "# iterator 를 반복하며 batch (src, tgt) 가 생성 됨.\n",
    "BATCH_SIZE = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 는 Batch 객체 (Tensor) 를 출력해주며, \n",
    "# Batch.src / Batch.tgt 로 parallel data각각에 대해 접근가능.\n",
    "\n",
    "# 예시.\n",
    "Batch = next(iter(train_iterator)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   3,  126,  803,   11,    2,    1,    1],\n",
       "        [   3,   28,    9,  132,   64,   98,    2],\n",
       "        [   3,   52, 6393,    2,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src 에 저장된 데이터 출력\n",
    "# Field에 정의된 형식으로 데이터 전처리 (indexing 포함.)\n",
    "# 가장 긴 문장을 기준으로, 그 보다 짧은 문장은 Padding idx(=1) 을 부여.\n",
    "Batch.src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    5,  141,   16,  775,    4,    3,    1,    1],\n",
       "        [   2,    5,   15,   74,    9, 1377,  153,    4,    3],\n",
       "        [   2,    5,    0,    4,    3,    1,    1,    1,    1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field에 정의된 형식으로 데이터 전처리 (indexing + bos + eos + pad 토큰 처리 됨.)\n",
    "Batch.tgt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder 정의.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, src_ntoken: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.src_ntoken = src_ntoken\n",
    "\n",
    "        self.embedding = nn.Embedding(src_ntoken, hidden_dim, \n",
    "                                      padding_idx=src.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, bidirectional = True, \n",
    "                          batch_first=True) # batch_first = [B, L, dim]\n",
    "        \n",
    "        \n",
    "        # bidirectional hidden을 하나의 hidden size로 mapping해주기 위한 Linear\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = (Batch, Length) Tensor\n",
    "        embedded = self.dropout(self.embedding(src)) # shape = (Batch, Length, hidden_dim)\n",
    "\n",
    "        # outputs: [B, L, D*2], hidden: [2, B, D] -> [1, B, D] + [1, B, D]\n",
    "        # Note: Bidirection=False 인 경우:  outputs: [B, L, D], hidden: [1, B, D]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        last_hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # bidirection Dim(x2)을 projection --> [B, D]\n",
    "        hidden = torch.tanh(last_hidden).unsqueeze(0) # last bidirectional hidden (=Decoder init hidden) --> [1, B, D]\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention 모듈 정의 ###\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        attn_in = (enc_hid_dim * 2) + dec_hid_dim # bidirectional hidden + dec_hidden\n",
    "        self.linear = nn.Linear(attn_in, attn_dim)\n",
    "        self.merge = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hiden = (Batch, 1, Dim) 길이가 1씩 들어오기 때문.\n",
    "        src_len = encoder_outputs.shape[1] \n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, src_len, 1) # [B, src_len, D] -> 각각의 src단어와 연산해주기 위해 늘려준 결과.\n",
    "\n",
    "        # enc의 각 step의 hidden + decoder의 hidden 의 결과값 # [B, src_len, D*2] --> [B, src_len, D]\n",
    "        # tanh(W*h_dec  + U*h_enc) 수식 부분.\n",
    "        energy = torch.tanh(self.linear(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2))) \n",
    "\n",
    "        score = self.merge(energy).squeeze(-1) # [B, src_len] 각 src 단어에 대한 점수 -> V^T tanh(W*h_dec  + U*h_enc) 부분\n",
    "        normalized_score = F.softmax(score, dim=1)  # softmax를 통해 확률분포값으로 변환\n",
    "        return  normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder 모듈 정의 ####\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, dec_ntoken: int, dropout: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim # Decoder RNN의 previous hidden\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(enc_hid_dim=hidden_dim, \n",
    "                                   dec_hid_dim=hidden_dim, \n",
    "                                   attn_dim=hidden_dim) # attention layer\n",
    "        \n",
    "        self.dec_ntoken = dec_ntoken # tgt vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(dec_ntoken, hidden_dim, \n",
    "                                      padding_idx=tgt.vocab.stoi['<pad>'])\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # bidirectinal=False 임.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(self.hidden_dim*3, dec_ntoken) # Vocab 크기로 linear projection\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # 확률 분포 값.\n",
    "\n",
    "    def _context_rep(self, dec_out, enc_outs):\n",
    "        scores = self.attention(dec_out, enc_outs) # score = [B, src_len]\n",
    "        scores = scores.unsqueeze(1) # [B, 1, src_len] -> weight value (softmax)\n",
    "\n",
    "        # scores: (batch, 1, src_len),  ecn_outs: (Batch, src_len, dim)\n",
    "        context_vector = torch.bmm(scores, enc_outs) # weighted average -> (batch, 1, dec_dim): encoder의 각 hidden의 weighted sum\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        dec_outs = []\n",
    "        embedded = self.dropout(self.embedding(input)) # (Batch, length, Dim)\n",
    "        \n",
    "        # (Batch, 1, dim)  (batch, 1, dim) , ....,\n",
    "        for emb_t in embedded.split(1, dim=1): # Batch 별 각 단어 (=각 time step) 에 대한 embedding 출력 \n",
    "            rnn_out, decoder_hidden = self.rnn(emb_t, decoder_hidden) # feed input with previous decoder hidden at each step\n",
    "\n",
    "            context = self._context_rep(rnn_out, encoder_outputs) # C_t vector\n",
    "            rnn_context = self.dropout(torch.cat([rnn_out, context], dim=2)) \n",
    "            dec_out = self.linear(rnn_context) # W(H + C) \n",
    "            dec_outs += [self.sm(dec_out)]\n",
    "        \n",
    "        if len(dec_outs) > 1:\n",
    "            dec_outs = dec_outs[:-1] # trg = trg[:-1] # <E> 는 Decoder 입력으로 고려하지 않음.\n",
    "            dec_outs = torch.cat(dec_outs, dim=1) # convert list into tensor : [B, L, vocab]\n",
    "\n",
    "        else: # step-wise 로 decoding 하는 경우,\n",
    "            dec_outs = dec_outs[0] # [B=1, L=1, vocab]\n",
    "\n",
    "        return dec_outs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq-to-Seq 모델 정의 ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden = self.encoder(src) # encoder_outputs = (Batch, length, Dim * 2) , hidden = (Batch, Dim)\n",
    "        dec_out, _ = self.decoder(trg, hidden, encoder_outputs)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src.vocab)  # src 사전 크기\n",
    "OUTPUT_DIM = len(tgt.vocab) # tgt 사전 크기\n",
    "HID_DIM = 128 # rnn, embedding, 등. 모든 hidden 크기를 해당 값으로 통일함. (실습의 용이성을 위함.)\n",
    "D_OUT = 0.1 # Dropout  확률\n",
    "BATCH_SIZE = 26\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 및 디코더 생성\n",
    "# Seq2Seq 모델 생성\n",
    "encoder = Encoder(HID_DIM, INPUT_DIM, D_OUT)\n",
    "decoder = Decoder(HID_DIM, OUTPUT_DIM, D_OUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights) # 모델 파라미터 초기화\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Optimizer 설정\n",
    "criterion = nn.NLLLoss(ignore_index=tgt.vocab.stoi['<pad>'], reduction='mean') # LOSS 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(13296, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (merge): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(15004, 128, padding_idx=1)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.1)\n",
      "    (linear): Linear(in_features=384, out_features=15004, bias=True)\n",
      "    (sm): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "The model has 9,778,461 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보 및 파라미터 수 출력\n",
    "def count_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 학습 함수 ###\n",
    "def train(model, iterator, optimize, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt) # [batch, length, vocab_size]\n",
    "        output = output.view(-1, output.size(-1)) # flatten --> (batch * length, vocab_size)\n",
    "\n",
    "        tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # 정답에는 <S>가 포함되지 않음으로, 이를 삭제\n",
    "        tgt = tgt.view(-1) # flatten = (batch * length)\n",
    "\n",
    "        loss = criterion(output, tgt) # tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if(((i+1) % int(len(iterator)*0.2)) == 0):\n",
    "            num_complete = batch.batch_size * (i+1)\n",
    "            total_size = batch.batch_size * int(len(iterator))\n",
    "            ratio = num_complete/total_size * 100\n",
    "            print('| Current Epoch:  {:>4d} / {:<5d} ({:2d}%) | Train Loss: {:3.3f}'.\n",
    "                  format(num_complete, batch.batch_size * int(len(iterator)), round(ratio), loss.item())\n",
    "                  )\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 평가 함수 ###\n",
    "def evaluate(model: nn.Module, iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "\n",
    "            output = model(src, tgt)\n",
    "            output = output.view(-1, output.size(-1)) # flatten (batch * length, vocab_size)\n",
    "\n",
    "            tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "            tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간 카운트를 위한 함수 #\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 4.373\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 4.032\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 3.680\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 3.494\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 3.023\n",
      "=================================================================\n",
      "Epoch: 01 | Time: 2m 1s\n",
      "\tTrain Loss: 3.855 | Train PPL:  47.239\n",
      "\t Val. Loss: 2.797 |  Val. PPL:  16.392\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 2.276\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 2.217\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.968\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 2.101\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.704\n",
      "=================================================================\n",
      "Epoch: 02 | Time: 2m 1s\n",
      "\tTrain Loss: 2.334 | Train PPL:  10.319\n",
      "\t Val. Loss: 1.924 |  Val. PPL:   6.852\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.500\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 2.058\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.525\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.394\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.770\n",
      "=================================================================\n",
      "Epoch: 03 | Time: 2m 1s\n",
      "\tTrain Loss: 1.700 | Train PPL:   5.472\n",
      "\t Val. Loss: 1.562 |  Val. PPL:   4.768\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.400\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.411\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.771\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.249\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.560\n",
      "=================================================================\n",
      "Epoch: 04 | Time: 2m 1s\n",
      "\tTrain Loss: 1.373 | Train PPL:   3.949\n",
      "\t Val. Loss: 1.382 |  Val. PPL:   3.982\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.458\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.090\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.457\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.985\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.121\n",
      "=================================================================\n",
      "Epoch: 05 | Time: 2m 1s\n",
      "\tTrain Loss: 1.174 | Train PPL:   3.234\n",
      "\t Val. Loss: 1.271 |  Val. PPL:   3.566\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.111\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 1.234\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.017\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.822\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 1.054\n",
      "=================================================================\n",
      "Epoch: 06 | Time: 2m 0s\n",
      "\tTrain Loss: 1.038 | Train PPL:   2.825\n",
      "\t Val. Loss: 1.200 |  Val. PPL:   3.320\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.902\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.766\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 1.029\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.879\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.931\n",
      "=================================================================\n",
      "Epoch: 07 | Time: 2m 1s\n",
      "\tTrain Loss: 0.941 | Train PPL:   2.564\n",
      "\t Val. Loss: 1.160 |  Val. PPL:   3.190\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.826\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.781\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.815\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 1.068\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.855\n",
      "=================================================================\n",
      "Epoch: 08 | Time: 2m 1s\n",
      "\tTrain Loss: 0.867 | Train PPL:   2.380\n",
      "\t Val. Loss: 1.131 |  Val. PPL:   3.099\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.583\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.863\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.916\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.856\n",
      "| Current Epoch:  59520 / 59568 (100%) | Train Loss: 1.215\n",
      "=================================================================\n",
      "Epoch: 09 | Time: 2m 1s\n",
      "\tTrain Loss: 0.807 | Train PPL:   2.242\n",
      "\t Val. Loss: 1.115 |  Val. PPL:   3.050\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.787\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.930\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.792\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.840\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.833\n",
      "=================================================================\n",
      "Epoch: 10 | Time: 2m 1s\n",
      "\tTrain Loss: 0.760 | Train PPL:   2.137\n",
      "\t Val. Loss: 1.103 |  Val. PPL:   3.014\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.619\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.904\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.798\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.553\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.808\n",
      "=================================================================\n",
      "Epoch: 11 | Time: 2m 1s\n",
      "\tTrain Loss: 0.719 | Train PPL:   2.052\n",
      "\t Val. Loss: 1.098 |  Val. PPL:   2.998\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 1.082\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.556\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.643\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.748\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.775\n",
      "=================================================================\n",
      "Epoch: 12 | Time: 2m 1s\n",
      "\tTrain Loss: 0.686 | Train PPL:   1.986\n",
      "\t Val. Loss: 1.092 |  Val. PPL:   2.979\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.592\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.841\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.524\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.706\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.751\n",
      "=================================================================\n",
      "Epoch: 13 | Time: 2m 1s\n",
      "\tTrain Loss: 0.659 | Train PPL:   1.932\n",
      "\t Val. Loss: 1.088 |  Val. PPL:   2.968\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.459\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.572\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.584\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.690\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.645\n",
      "=================================================================\n",
      "Epoch: 14 | Time: 2m 1s\n",
      "\tTrain Loss: 0.633 | Train PPL:   1.883\n",
      "\t Val. Loss: 1.090 |  Val. PPL:   2.975\n",
      "=================================================================\n",
      "| Current Epoch:  25792 / 129064 (20%) | Train Loss: 0.781\n",
      "| Current Epoch:  51584 / 129064 (40%) | Train Loss: 0.717\n",
      "| Current Epoch:  77376 / 129064 (60%) | Train Loss: 0.708\n",
      "| Current Epoch:  103168 / 129064 (80%) | Train Loss: 0.649\n",
      "| Current Epoch:  128960 / 129064 (100%) | Train Loss: 0.845\n",
      "=================================================================\n",
      "Epoch: 15 | Time: 2m 1s\n",
      "\tTrain Loss: 0.611 | Train PPL:   1.842\n",
      "\t Val. Loss: 1.091 |  Val. PPL:   2.977\n",
      "=================================================================\n",
      "model saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/wklee/envs/torch_1.0/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/wklee/envs/torch_1.0/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/wklee/envs/torch_1.0/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/wklee/envs/torch_1.0/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15 # 최대 epoch 크기\n",
    "CLIP = 0.2 # weight cliping \n",
    "isTrain = True # True 인 경우 아래 학습 코드 실행, False인 경우 저장된 model 로드만 수행.\n",
    "\n",
    "if isTrain:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        print('='*65)\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        print('='*65)\n",
    "\n",
    "    with open('NMT.pt', 'wb') as f:\n",
    "        print(\"model saving..\")\n",
    "        torch.save(model, f)\n",
    "\n",
    "else:\n",
    "    with open('NMT.pt', 'rb') as f:\n",
    "        model = torch.load(f).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy decoding \n",
    "def greedy_decoding(model, input, fields, maxLen=20):\n",
    "    src_field = [('src', fields[0])]\n",
    "    tgt_field = fields[1]\n",
    "\n",
    "    ex = Example.fromlist([input], src_field) # field에 정의된 내용으로 전처리 (tokenizing) 수행\n",
    "    src_tensor = src.numericalize([ex.src], device) # torch.Tensor로 치환, indexing, bos, eos 등의 처리과정도 함께 적용됨.\n",
    "    tgt_tensor = torch.tensor([[tgt_field.vocab.stoi['<s>']]], device=device) # Decoder 초기 입력 \n",
    "    model.eval()\n",
    "\n",
    "    dec_result = []\n",
    "    with torch.no_grad():\n",
    "        enc_out, hidden = model.encoder(src_tensor)\n",
    "        for i in range(maxLen):\n",
    "            # Step1: tgt_tensor (입력) 과 인코더의 출력을 이용하여 디코더 결과 출력\n",
    "            # Do someting about Step1 here..\n",
    "            # --> dec_step, hidden = model.decoder(....)\n",
    "            \n",
    "            # Step2: 디코더의 출력결과 (확룰분포) 에서 Top1 에 해당하는 word Index 추출\n",
    "            # Do someting about Step2 here..\n",
    "            # use torch.topk(..) \n",
    "            \n",
    "            # Step3: \n",
    "            # if: 출력된 word Index == EOS 인 경우 디코딩 중지 (break).\n",
    "            # else: 출력된 word Index를 저장하고, 다음 step의 디코더 입력 (tgt_tensor)으로 전달\n",
    "           \n",
    "    dec_result = [tgt_field.vocab.itos[w] for w in dec_result] # Word index를 단어로 치환\n",
    "    return dec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter english sentence:  I went to the theater with her last night.\n",
      ">  I went to the theater with her last night.\n",
      "<  je suis allé au cinéma avec elle la nuit dernière.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding 수행\n",
    "\n",
    "input_sent = input('Enter a english sentence:  ')\n",
    "output = greedy_decoding(model, input_sent, fields=(src, tgt))\n",
    "output = MosesDetokenizer('fr')(output)\n",
    "print('> ', input_sent)\n",
    "print('< ', output)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.4",
   "language": "python",
   "name": "torch_1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
