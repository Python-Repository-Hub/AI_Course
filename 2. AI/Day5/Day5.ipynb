{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util, math, random\n",
    "from collections import defaultdict\n",
    "from util import ValueIteration\n",
    "\n",
    "_X_ = None\n",
    "\n",
    "############################################################\n",
    "# Problem A\n",
    "\n",
    "class ExampleMDP(util.MDP):\n",
    "    def startState(self):\n",
    "        return 0\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def actions(self, state):\n",
    "        return ['Left', 'Right']\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.\n",
    "    def succAndProbReward(self, state, action):\n",
    "        if state == -2 or state == 2:\n",
    "            return []\n",
    "        \n",
    "        leftReward = -5\n",
    "        rightReward = -5\n",
    "\n",
    "        if state - 1 == -2:\n",
    "            leftReward = 20\n",
    "        if state + 1 == 2:\n",
    "            rightReward = 100\n",
    "        \n",
    "        if action == 'Left':\n",
    "            results = [(state-1, 0.8, leftReward), (state+1, 0.2, rightReward)]\n",
    "        elif  action == 'Right':\n",
    "            results = [(state-1, 0.7, leftReward), (state+1, 0.3, rightReward)]\n",
    "        else:\n",
    "            results = []\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    def discount(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Problem C\n",
    "\n",
    "class BlackjackMDP(util.MDP):\n",
    "    def __init__(self, cardValues, multiplicity, threshold, peekCost):\n",
    "        \"\"\"\n",
    "        cardValues: array of card values for each card type\n",
    "        multiplicity: number of each card type\n",
    "        threshold: maximum total before going bust\n",
    "        peekCost: how much it costs to peek at the next card\n",
    "        \"\"\"\n",
    "        self.cardValues = cardValues\n",
    "        self.multiplicity = multiplicity\n",
    "        self.threshold = threshold\n",
    "        self.peekCost = peekCost\n",
    "\n",
    "    # Return the start state.\n",
    "    # Look at this function to learn about the state representation.\n",
    "    # The first element of the tuple is the sum of the cards in the player's\n",
    "    # hand.\n",
    "    # The second element is the index (not the value) of the next card, if the player peeked in the\n",
    "    # last action.  If they didn't peek, this will be None.\n",
    "    # The final element is the current deck.\n",
    "    def startState(self):\n",
    "        return (0, None, (self.multiplicity,) * len(self.cardValues))  # total, next card (if any), multiplicity for each card\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    # You do not need to modify this function.\n",
    "    # All logic for dealing with end states should be done in succAndProbReward\n",
    "    def actions(self, state):\n",
    "        return ['Take', 'Peek', 'Quit']\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.  Indicate a terminal state (after quitting or\n",
    "    # busting) by setting the deck to None. \n",
    "    # When the probability is 0 for a particular transition, don't include that \n",
    "    # in the list returned by succAndProbReward.\n",
    "    def succAndProbReward(self, state, action):\n",
    "        # BEGIN_YOUR_CODE\n",
    "        #raise NotImplementedError  # remove or comment this line\n",
    "        succ_prob_reward_list = []\n",
    "        card_sum, peek_idx, deck = state  # card_sum = the sum of taken cards' values\n",
    "\n",
    "        if deck is None:  # when there is no card in the deck\n",
    "            pass          # no possible successor state\n",
    "\n",
    "        elif action == 'Take':\n",
    "            num_all_cards = sum(deck)  # the number of all cards\n",
    "\n",
    "            # get_succ_reward(idx) returns a successor state and a reward, when a card is taken.\n",
    "            def get_succ_reward(idx):\n",
    "                new_card_sum = card_sum + self.cardValues[idx]  # what's the new sum of card values, when we take a new card?\n",
    "                if new_card_sum > self.threshold:  # when the card sum exceeds the threshold\n",
    "                    new_deck = None\n",
    "                    reward = 0\n",
    "                elif num_all_cards > 1:  # sum(new_deck) > 0; when some cards remain\n",
    "                    new_deck = list(deck)       # it may need multiple lines\n",
    "                    new_deck[idx] -= 1\n",
    "                    new_deck = tuple(new_deck)\n",
    "                    reward = 0\n",
    "                else:  # when there is no card remaining\n",
    "                    new_deck = None\n",
    "                    reward = new_card_sum\n",
    "                succ = new_card_sum, None, new_deck\n",
    "                return succ, reward\n",
    "\n",
    "            # Peek implementation ----------------------------------------\n",
    "            if peek_idx is not None:  # when previous action was 'Peek'\n",
    "                succ, reward = get_succ_reward(peek_idx)\n",
    "                succ_prob_reward_list.append((succ, 1, reward))\n",
    "            # ---------------------------------------- Peek implementation\n",
    "            else:  # when previous action was not 'Peek'\n",
    "                for idx, num in enumerate(deck):\n",
    "                    if num == 0:\n",
    "                        continue                        \n",
    "                    succ, reward = get_succ_reward(idx)\n",
    "                    prob = num / num_all_cards\n",
    "                    succ_prob_reward_list.append((succ, prob, reward))\n",
    "\n",
    "        # Peek implementation ----------------------------------------\n",
    "        elif action == 'Peek':\n",
    "            if peek_idx is None:\n",
    "                num_all_cards = sum(deck)\n",
    "\n",
    "                for idx, num in enumerate(deck):\n",
    "                    if num == 0:\n",
    "                        continue\n",
    "                    prob = num / num_all_cards\n",
    "                    succ_prob_reward_list.append(((card_sum, idx, deck), prob, - self.peekCost))\n",
    "        # ---------------------------------------- Peek implementation\n",
    "\n",
    "        elif action == 'Quit':\n",
    "            succ_prob_reward_list.append(((card_sum, None, None), 1, card_sum))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Undefined action '{}'\".format(action))\n",
    "\n",
    "        return succ_prob_reward_list\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "    def discount(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem D: Q learning\n",
    "\n",
    "# Performs Q-learning.  Read util.RLAlgorithm for more information.\n",
    "# actions: a function that takes a state and returns a list of actions.\n",
    "# discount: a number between 0 and 1, which determines the discount factor\n",
    "# featureExtractor: a function that takes a state and action and returns a list of (feature name, feature value) pairs.\n",
    "# explorationProb: the epsilon value indicating how frequently the policy\n",
    "# returns a random action\n",
    "class QLearningAlgorithm(util.RLAlgorithm):\n",
    "    def __init__(self, actions, discount, featureExtractor, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.explorationProb = explorationProb\n",
    "        self.weights = defaultdict(float)\n",
    "        self.numIters = 0\n",
    "\n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state, action):\n",
    "        score = 0\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            score += self.weights[f] * v\n",
    "        return score\n",
    "\n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state):\n",
    "        self.numIters += 1\n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self):\n",
    "        return 1.0 / math.sqrt(self.numIters)\n",
    "\n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state, action, reward, newState):\n",
    "        # BEGIN_YOUR_CODE\n",
    "        #raise NotImplementedError  # remove or comment this line\n",
    "        if newState is None:\n",
    "            v_opt = 0\n",
    "        else:\n",
    "            v_opt = max(self.getQ(newState, a) for a in self.actions(newState))  # v_opt(s')\n",
    "        diff = self.getQ(state, action) - (reward + self.discount * v_opt)\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            eta = self.getStepSize()\n",
    "            self.weights[f] -= eta*diff*v\n",
    "        # END_YOUR_CODE\n",
    "\n",
    "# Return a singleton list containing indicator feature for the (state, action)\n",
    "# pair.  Provides no generalization.\n",
    "def identityFeatureExtractor(state, action):\n",
    "    featureKey = (state, action)\n",
    "    featureValue = 1\n",
    "    return [(featureKey, featureValue)]\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem E: convergence of Q-learning\n",
    "\n",
    "def compareQLandVI(targetMDP, featureExtractor):\n",
    "    QL = QLearningAlgorithm(targetMDP.actions, 1, featureExtractor)\n",
    "    VI = ValueIteration()\n",
    "    \n",
    "    util.simulate(targetMDP, QL, numTrials=30000)\n",
    "    VI.solve(targetMDP)\n",
    "\n",
    "    diffPolicyStates = []\n",
    "    QL.explorationProb = 0\n",
    "    for state in targetMDP.states:\n",
    "        #print state, QL.getAction(state), VI.pi[state]\n",
    "        if QL.getAction(state) != VI.pi[state]:\n",
    "            diffPolicyStates.append(state)\n",
    "    print(\"%d/%d = %f%% different states\"%(len(diffPolicyStates), len(targetMDP.states), len(diffPolicyStates)/float(len(targetMDP.states))))\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Problem F: features for Q-learning.\n",
    "\n",
    "# You should return a list of (feature key, feature value) pairs (see\n",
    "# identityFeatureExtractor()).\n",
    "# Implement the following features:\n",
    "# - indicator on the total and the action (1 feature).\n",
    "# - indicator on the presence/absence of each card and the action (1 feature).\n",
    "#       Example: if the deck is (3, 4, 0 , 2), then your indicator on the presence of each card is (1,1,0,1)\n",
    "#       Only add this feature if the deck != None\n",
    "# - indicator on the number of cards for each card type and the action (len(counts) features).  Only add these features if the deck != None\n",
    "\n",
    "\n",
    "def blackjackFeatureExtractor(state, action):\n",
    "    total, nextCard, counts = state\n",
    "    ## - indicator on the total and the action (1 feature).\n",
    "    features = [((total, action), 1)]    ## - indicator on the presence/absence of each card and the action (1 feature).\n",
    "    if counts is not None:\n",
    "        zero_one_list=[]\n",
    "        for x in counts:\n",
    "            if x!=0:\n",
    "                zero_one_list.append(1)\n",
    "            else:\n",
    "                zero_one_list.append(0)\n",
    "        features.append(((tuple(zero_one_list), action),1))    ## - indicator on the number of cards for each card type and the action (len(counts) features).\n",
    "    if counts is not None:\n",
    "        for i in range(len(counts)):\n",
    "            features.append(((i, counts[i], action), 1))    \n",
    "    \n",
    "    return features    \n",
    "\n",
    "\n",
    "############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Problem F ==========\n",
      "Q-value for (state = (7, None, (0, 1)), action = 'Quit') : Answer 28.0, Output 28.0\n",
      "Q-value for (state = (7, None, (1, 0)), action = 'Quit') : Answer 7.0, Output 7.0\n",
      "Q-value for (state = (2, None, (0, 2)), action = 'Quit') : Answer 14.0, Output 14.0\n",
      "Q-value for (state = (2, None, (0, 2)), action = 'Take') : Answer 0.0, Output 0.0\n",
      "591/2745 = 0.215301% different states\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from submission import *\n",
    "from util import *\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "#         print('\\n========== Problem A ==========')\n",
    "#         mdp = ExampleMDP()\n",
    "#         algorithm = ValueIteration()\n",
    "#         algorithm.solve(mdp, 20, verbose=True) # when epsilon=20, the algorithm repeats 2 iterations\n",
    "#         for i in [-2, -1, 0, 1, 2]:\n",
    "#             print(\"Value of the state '%d' : %f\"%(i, algorithm.V[i]))\n",
    "\n",
    "#         for i in [-1, 0, 1]:\n",
    "#             print(\"Policy at the state '%d' : %s\"%(i, algorithm.pi[i]))\n",
    "\n",
    "#         print('\\n========== Problem C ==========')\n",
    "#         mdp1 = BlackjackMDP(cardValues=[1, 5], multiplicity=2, threshold=10, peekCost=1)\n",
    "#         startState = mdp1.startState()\n",
    "#         preBustState = (6, None, (1, 1))\n",
    "#         postBustState = (11, None, None)\n",
    "\n",
    "#         mdp2 = BlackjackMDP(cardValues=[1, 5], multiplicity=2, threshold=15, peekCost=1)\n",
    "#         preEmptyState = (11, None, (1,0))\n",
    "\n",
    "#         print('\\n---------- Test c1 ----------')\n",
    "#         # Make sure the succAndProbReward function is implemented correctly.\n",
    "\n",
    "#         vanilla_tests = [\n",
    "#             ([((1, None, (1, 2)), 0.5, 0), ((5, None, (2, 1)), 0.5, 0)], mdp1, startState, 'Take'),\n",
    "#             ([((0, None, None), 1, 0)], mdp1, startState, 'Quit'),\n",
    "#             ([((7, None, (0, 1)), 0.5, 0), ((11, None, None), 0.5, 0)], mdp1, preBustState, 'Take'),\n",
    "#             ([], mdp1, postBustState, 'Take'),\n",
    "#             ([], mdp1, postBustState, 'Quit'),\n",
    "#             ([((12, None, None), 1., 12)], mdp2, preEmptyState, 'Take'),\n",
    "#         ]\n",
    "\n",
    "#         print('Vanilla Blackjack')\n",
    "#         for no, (answer, mdp, state, action) in enumerate(vanilla_tests):\n",
    "#             print('No %d'%(no+1), end=' ')\n",
    "#             if answer != mdp.succAndProbReward(state, action):\n",
    "#                 print('=> wrong')\n",
    "#             else:\n",
    "#                 print('=> right')\n",
    "#             print('- state: {}, action: {}'.format(state, action))\n",
    "#             print('- true answer =', answer)\n",
    "#             print('- your answer =', mdp.succAndProbReward(state, action))\n",
    "\n",
    "#         print('\\n---------- Test c2 ----------')\n",
    "#         peek_tests = [\n",
    "#             ([((0, 0, (2, 2)), 0.5, -1), ((0, 1, (2, 2)), 0.5, -1)], mdp1, startState, 'Peek'),\n",
    "#             ([((1 , None, (1, 2) ), 1, 0)] , mdp1, (0, 0, (2, 2)), 'Take'),\n",
    "#             ([], mdp1, postBustState, 'Peek'),\n",
    "#             ]\n",
    "\n",
    "#         print('Peeking Blackjack')\n",
    "#         for no, (answer, mdp, state, action) in enumerate(peek_tests):\n",
    "#             print('No %d'%(no+1), end=' ')\n",
    "#             if answer != mdp.succAndProbReward(state, action):\n",
    "#                 print('=> wrong')\n",
    "#             else:\n",
    "#                 print('=> right')\n",
    "#             print('- state: {}, action: {}'.format(state, action))\n",
    "#             print('- true answer =', answer)\n",
    "#             print('- your answer = ', mdp.succAndProbReward(state, action))\n",
    "\n",
    "#         print('\\n---------- Test c3 ----------')\n",
    "#         algorithm = ValueIteration()\n",
    "#         algorithm.solve(mdp1, verbose=True)\n",
    "#         for s in algorithm.V:\n",
    "#             print('V(%s) = %f'%(s, algorithm.V[s]))\n",
    "#         print('------------')\n",
    "#         for s in algorithm.pi:\n",
    "#             print('pi(%s) = %s'%(s, algorithm.pi[s]))\n",
    "#         print('------------')\n",
    "#         print('Q1 (6, None, (1, 1) => %s'%(algorithm.pi[(6, None, (1, 1))]))\n",
    "#         print('Q2 (6, 0, (1, 1) => %s'%(algorithm.pi[(6, 0, (1, 1))]))\n",
    "\n",
    "#         print('\\n========== Problem D ==========')\n",
    "#         mdp = util.NumberLineMDP()\n",
    "#         rl = QLearningAlgorithm(mdp.actions, mdp.discount(), identityFeatureExtractor, 0)\n",
    "\n",
    "#         # We call this here so that the stepSize will be 1\n",
    "#         rl.numIters = 1\n",
    "\n",
    "#         rl.incorporateFeedback(0, 1, 0, 1)\n",
    "#         print('Q-value for (state = 0, action = -1) : Answer %.1f, Output %.1f'%(0, rl.getQ(0, -1)))\n",
    "#         print('Q-value for (state = 0, action =  1) : Answer %.1f, Output %.1f'%(0, rl.getQ(0, 1)))\n",
    "\n",
    "#         rl.incorporateFeedback(1, 1, 1, 2)\n",
    "#         print('Q-value for (state = 0, action = -1) : Answer %.1f, Output %.1f'%(0, rl.getQ(0, -1)))\n",
    "#         print('Q-value for (state = 0, action =  1) : Answer %.1f, Output %.1f'%(0, rl.getQ(0, 1)))\n",
    "#         print('Q-value for (state = 1, action = -1) : Answer %.1f, Output %.1f'%(0, rl.getQ(1, -1)))\n",
    "#         print('Q-value for (state = 1, action =  1) : Answer %.1f, Output %.1f'%(1, rl.getQ(1, 1)))\n",
    "\n",
    "#         rl.incorporateFeedback(2, -1, 1, 1)\n",
    "#         print('Q-value for (state = 2, action = -1) : Answer %.1f, Output %.1f'%(1.9, rl.getQ(2, -1)))\n",
    "#         print('Q-value for (state = 2, action =  1) : Answer %.1f, Output %.1f'%(0, rl.getQ(2, 1)))\n",
    "\n",
    "#         print('\\n========== Problem E ==========')\n",
    "#         # Small test case\n",
    "#         smallMDP = BlackjackMDP(cardValues=[1, 5], multiplicity=2, threshold=10, peekCost=1)\n",
    "#         compareQLandVI(smallMDP, identityFeatureExtractor)\n",
    "\n",
    "#         # Large test case\n",
    "#         largeMDP = BlackjackMDP(cardValues=[1, 3, 5, 8, 10], multiplicity=3, threshold=40, peekCost=1)\n",
    "#         compareQLandVI(largeMDP, identityFeatureExtractor)\n",
    "        print('\\n========== Problem F ==========')\n",
    "\n",
    "        mdp = BlackjackMDP(cardValues=[1, 5], multiplicity=2, threshold=10, peekCost=1)\n",
    "        rl = QLearningAlgorithm(mdp.actions, mdp.discount(), blackjackFeatureExtractor, 0)\n",
    "\n",
    "        # We call this here so that the stepSize will be 1\n",
    "        rl.numIters = 1\n",
    "\n",
    "        rl.incorporateFeedback((7, None, (0, 1)), 'Quit', 7, (7, None, None))\n",
    "        print(\"Q-value for (state = (7, None, (0, 1)), action = 'Quit') : Answer %.1f, Output %.1f\"%(28, rl.getQ((7, None, (0, 1)), 'Quit')))\n",
    "        print(\"Q-value for (state = (7, None, (1, 0)), action = 'Quit') : Answer %.1f, Output %.1f\"%(7, rl.getQ((7, None, (1, 0)), 'Quit')))\n",
    "        print(\"Q-value for (state = (2, None, (0, 2)), action = 'Quit') : Answer %.1f, Output %.1f\"%(14, rl.getQ((2, None, (0, 2)), 'Quit')))\n",
    "        print(\"Q-value for (state = (2, None, (0, 2)), action = 'Take') : Answer %.1f, Output %.1f\"%(0, rl.getQ((2, None, (0, 2)), 'Take')))\n",
    "\n",
    "        # Large test case\n",
    "        largeMDP = BlackjackMDP(cardValues=[1, 3, 5, 8, 10], multiplicity=3, threshold=40, peekCost=1)\n",
    "        random.seed(0)\n",
    "        compareQLandVI(largeMDP, blackjackFeatureExtractor)  # 591/2745 = 0.215301% different states (when random.seed(0))\n",
    "\n",
    "    except NotImplementedError as err:\n",
    "        # print err\n",
    "        print(\"\\nNotImplementedError: you didn't implement the function.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
